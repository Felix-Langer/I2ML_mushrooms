---
title: "Introduction to Mushroom Learning"
author: "Andreas Kla√ü, Cornelia Gruber, Felix Langer, Viktoria Szabo"
date: "15.05.2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r, echo=FALSE,fig.align = 'center',out.width="100%",fig.cap="source: https://blog.goodeggs.com/blog/cooking-different-types-of-mushrooms"}
knitr::include_graphics("images/mushrooms1.jpg")
```


Collecting mushrooms recently gained popularity (which you can see on Instagram and mushroom picking blogs). Since mushrooms come in many different colors and shapes, their fit or unfit for human consumption is not necessarily obvious. Naturally, instead of asking your grandparents for advice a statistical analysis is the way to go to find out whether a mushroom is edible or poisonous given attributes concerning the look or smell of a mushroom.

### Mushroom attributes
Our main goal is to classify each mushroom into one of the two classes "edible" or "poisonous" by using machine learning methods. For this task we can use various attributes of a mushroom's cap, gill or stalk or attributes like the odor, population or habitat of the mushroom. The cap shape for example can be either "bell", "conical", "convex", "flat", "knobbed" or "sunken" (see figure below). The dataset contains 8124 observations with 22 nominal features.

```{r, echo=FALSE,out.width="49%",fig.cap="source: https://www.mushroomdiary.co.uk/mushroom-identification/",fig.show='hold',fig.align='center'}

knitr::include_graphics(c("images/parts.jpg","images/cap_shape.jpg"))

```

### Dataset Overview
Variable | Encoding
---------|---------
classes| edible=e,  poisonous=p
cap-shape| bell=b, conical=c, convex=x, flat=f,  knobbed=k, sunken=s  
cap-surface| fibrous=f, grooves=g, scaly=y, smooth=s  
cap-color| brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y  
bruises| bruises=t, no=f  
odor| almond=a, anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s  
gill-attachment| attached=a, descending=d, free=f, notched=n  
gill-spacing| close=c, crowded=w, distant=d  
gill-size| broad=b, narrow=n  
gill-color| black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y  
stalk-shape| enlarging=e, tapering=t  
stalk-root| bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?  
stalk-surface-above-ring| fibrous=f, scaly=y, silky=k, smooth=s  
stalk-surface-below-ring| fibrous=f, scaly=y, silky=k, smooth=s  
stalk-color-above-ring| brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y  
stalk-color-below-ring| brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y  
veil-type| partial=p, universal=u  
veil-color| brown=n, orange=o, white=w, yellow=y  
ring-number| none=n, one=o, two=t  
ring-type| cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z  
spore-print-color| black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y  
population| abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y  
habitat| grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d   



```{r,echo=FALSE, message=FALSE, warning=FALSE}
# load packages and data
library(tidyverse)
library(mlr3verse)
library(ranger) #random forests
library(gridExtra) #plotting 
library(precrec)
set.seed(123456)

# capture current ggplot theme and reset it at the end of the script:
original_ggplot_theme <- ggplot2::theme_get()
# set ggplot theme for this script:
ggplot2::theme_set(ggplot2::theme_bw())

#mushrooms for plotting with different factor lables, mushroom_data for modelling
mushrooms_data = read.csv("Data/mushrooms.csv") %>% 
  select( - veil.type)  # veil.type has only 1 level => omit variable
mushrooms = mushrooms_data
  
# setting var labels for plotting
mushrooms$class <- factor(mushrooms$class, labels = c("edible", "poisonous"))
mushrooms$odor <- factor(mushrooms$odor, labels = c("almond", "creosote", "foul", "anise", "musty", "none", "pungent", "spicy", "fishy"))
mushrooms$gill.color <- factor(mushrooms$gill.color, 
                         labels = c("buff", "red", "gray", "chocolate", 
                                     "black", "brown", "orange",
                                    "pink", "green", "purple", "white", "yellow"))
```


```{r}
#Overview of data
head(mushrooms_data)

summary(mushrooms_data)
```

In order to get a better feeling for our mushrooms let's do some plotting:   
Odor seems to be quite good indicator whether a mushroom is poisonous or edible. For example, a purgant, spicy or fishy odor clearly identifies poisonous mushrooms.          
```{r, echo=FALSE}
p1 <- ggplot(mushrooms, aes(x= odor, fill = class)) +
  geom_bar() +
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 45,  hjust = 1))+
  ggtitle("Distribution of class labels - odor")+
  scale_fill_manual(values = c("#00BFC4", "#F8766D"))

p2 <- ggplot(mushrooms, aes(x = gill.color, fill = class)) +
  geom_bar()+
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 45,  hjust = 1))+
  ggtitle("Distribution of class labels - gill.color")+
  scale_fill_manual(values = c("#00BFC4", "#F8766D"))

grid.arrange(p1, p2, ncol =2)

```

### Mushroom Learning
The machine learning analysis focuses in particular on two aspects: Fist comparing different machine learning methods such as KNN or Random Forests on this specific data set and second including hyper parameter tuning for some of the used methods.     
#### Evaluation Framework

mlr3 is a machine learning (our in our case mushroom learning) framework in R offering a common ground for all necessary steps as defining a task, training a learner, predicting new data and resampling.    
A task in mlr3 contains the data as well as meta information such as the name of the target variable or if its a "classification" "regression" task.
```{r}
# Construct Classification Task 
task_mushrooms = TaskClassif$new(id = "mushrooms_data",
                               backend = mushrooms_data,
                               target = "class",
                               positive = "e") # "e" = edible
```


Additionally to building a machine learning model for predicting the classes it is crucial to obtain a realistic generalization error for our estimates. Therefore we decided for a nested resampling strategy with a 5-fold CV in inner loop and a 10-fold CV in the outer loop. The number of folds was chosen in a way to balance the bias and variance of our estimate while still considering run time.   

```{r}
# Resampling Strategies 
# 5 fold cross validation for inner loop
resampling_inner_5CV = rsmp("cv", folds = 5L)
# 10 fold cross validation for outer loop
resampling_outer_10CV = rsmp("cv", folds = 10L)
```

While model tuning will be based on the AUC in order to classify most observations correctly, printing other measures is useful for assessing the performances of other aspects such as falsely predicting a poisonous mushroom as edible.   
For the second aspect of the project, hyperparameter tuning, we chose grid search because the range of possible hyperparameter values is discrete and rather small. More precisely, the hyperparameter mtry for random forest (number of variables randomly sampled as candidates at each split)  may be between 1 and 21 and k in KNN (number of neighbors considered) may be between 1 and 50.    

```{r}
# Performance Measures 
measures = list(
  msr("classif.auc",
      id = "AUC"),
  msr("classif.fpr",
      id = "False Positive Rate"), # false positive rate especially interesting
  # for our falsely edible (although actually poisonous) classification
  msr("classif.sensitivity",
      id = "Sensitivity"),
  msr("classif.specificity",
      id = "Specificity"),
  msr("classif.ce", 
      id = "MMCE")
)

# Choose optimization algorithm:
tuner_grid_search_knn = tnr("grid_search", resolution = 50)
tuner_grid_search_mtry = tnr("grid_search", resolution = 21)

# evaluate performance on AUC:
measures_tuning = msr("classif.auc", id = "AUC")
# Set when to terminate:
terminator_knn = term("evals", n_evals = 50)
# since almost all mtry values lead to very good results we evaluate 1 to 21 features as opposed to a termination criterion like stagnation
terminator_mtry = term("evals", n_evals = 21)  


```

#### Choosing algorithms
A relevant property of our dataset is that all features are nominal and thus multinomial distributed. Therefore linear and quadratic discriminant analyses (LDA and QDA) are due to the violated assumption of normally distributed features not possible.    
Following the performance of featureless, naive bayes, KNN, logistic regression, decision tree and random forest models will be compared since they are all suitable for binary classification with nominal features.     

In the following part we define our inner loop of nested resampling for tuning hyperparameters. Only random forest (mtry) and KNN (k) have hyperparameters which will be tuned in a 5-fold-cv.

```{r}

# Autotune knn -----------------------------------------------------------------
# Define learner:
learner_knn = lrn("classif.kknn", predict_type = "prob")

# tune the chosen hyperparameter k with these boundaries:
param_k = ParamSet$new(
  list(
    ParamInt$new("k", lower = 1L, upper = 50)
  )
)

# Set up autotuner instance with the predefined setups
tuner_knn = AutoTuner$new(
  learner = learner_knn,
  resampling = resampling_inner_5CV,
  measures = measures_tuning, 
  tune_ps = param_k, 
  terminator = terminator_knn,
  tuner = tuner_grid_search_knn
)



# Autotune Random Forest ---------------------------------------------------------------------------
# Define learner:
learner_ranger = lrn("classif.ranger", predict_type = "prob", importance = "impurity")


# we will try all configurations: 1 to 21 features.
param_mtry = ParamSet$new(
  list(
    ParamInt$new("mtry", lower = 1L, upper = 21L)
  ) 
)

# Set up autotuner instance with the predefined setups
tuner_ranger = AutoTuner$new(
  learner = learner_ranger,
  resampling = resampling_inner_5CV,
  measures = measures_tuning,
  tune_ps = param_mtry, 
  terminator = terminator_mtry,
  tuner = tuner_grid_search_mtry
)


(learners = list(lrn("classif.featureless", predict_type = "prob"),
                lrn("classif.naive_bayes", predict_type = "prob"),
                lrn("classif.rpart", predict_type = "prob"),
                lrn("classif.log_reg", predict_type = "prob"),
                tuner_ranger,
                tuner_knn))
```


#### Benchmark Results
The final benchmark of all 6 methods, which is the outer loop of our nested resampling with a 10-fold-cv, is shown below. In this step the generalization error as well as other performance measures of the algorithms are compared.   
```{r, results='hide'}
design = benchmark_grid(
  tasks = task_mushrooms,
  learners = learners,
  resamplings = resampling_outer_10CV
)



bmr = benchmark(design, store_models = TRUE)

```
When nominal features are included in a logistic regression each category is binary encoded, however if in one category every single observation has the same target variable class (edible or poisonous) the model does not converge. This means that the estimated regression coefficient would need to be infinity, since the probability for an observation to belong to a specific class is exactly 1. As this is what happens for some categories such as "spicy odor" the model fit is not quite stable and interpreting the coefficients might be misleading.       

Beyond this, we see that no errors or warnings occurred during resampling when printing the benchmark result print(bmr).

```{r}

print(bmr)
```


In the following table you can see how every algorithm (excluding featureless) leads to nearly perfect classification results and thus to an AUC to 1 or almost 1. As expected a featureless model stays at a classification error of close to 0.5 since the most common class label will be predicted for every observation regardless of features and our classes are almost equally distributed (51.8% edible vs. 48.2% poisonous). The logistic regression (log_reg), random forest (ranger) and KNN (kknn) all have an AUC of 1.0000000. However, if you look at the False Positive Rate (FPR) of KNN it is not exactly zero and therefore the AUC cannot be exactly 1 and has been rounded. Even a, otherwise rather unstable, decision tree (rpart) leads to incredible results and a FPR of only 1.2%. Those results are however not surprising since there are many features which can separate the classes very well. Therefore there is no real need to tune hyperparameters or choose a complex model.         
```{r}
tab_learner_performance = bmr$aggregate(measures)
tab_learner_performance[,learner_id:MMCE]
```



In the first plot you can see how the classification error is distributed among the different cv folds for the different algorithms and in the second plot the ROC curve is displayed.   
```{r}
autoplot(bmr)
autoplot(bmr, type = "roc")


```




### Final model
All steps so far answered the question which model with which hyperparameters yields good results with a now known generalization error. However to obtain the best possible predictions, training with all available data points is needed.
Our previous models showed:      
* Naive Bayes has a worse FPR than other models, which is relevant considering a mushroom which is falsely classified as edible might lead to poisoning or even death
* Perfect classification results for random forest and logistic regression
* Logistic regression has convergence issues
* Almost perfect classification for KNN and decision tree

Even though a single tree would be the most useful and applicable model when picking mushrooms in real life, we chose the random forest as final model since it can perfectly classify all observations and has no stability issues such as the logistic regression. 


```{r, results = 'hide'}
# Train tuner_ranger once again using the same specs as before
tuner_ranger$tuning_instance

# show only warnings:
lgr::get_logger("mlr3")$set_threshold("warn")
tuner_ranger$train(task_mushrooms)
# reset console outputs to default:
lgr::get_logger("mlr3")$set_threshold("info")

# AUC performance for parameter combinations:
tuner_ranger$tuning_instance$archive(unnest = "params")[,
                                                        c("mtry","AUC")] %>% 
  arrange(mtry)
# Pretty much every combination works perfectly

tuner_ranger$tuning_result # winning mtry is 3 although we could use
# 2-21 and achieve the same perfect performance

# use these parameters for our final winner model with winner specs:
learner_final = lrn("classif.ranger",predict_type = "prob")
learner_final$param_set$values = tuner_ranger$tuning_result$params

# Fit winner model to entire data set
learner_final$train(task_mushrooms)

```
To prevent the random forest from remaining just a black box algorithm, the variable importance is shown in the plot below. As suspected in the descriptive part, odor is clearly a good indicator for edibility of a mushroom.
```{r}
# construct filter to extract variable importance in previously set up winning learner
filter_ranger = flt("importance", learner = learner_final)
# rerun learner on entire data set and store variable importance results
filter_ranger$calculate(task_mushrooms)

feature_scores <- as.data.table(filter_ranger)

ggplot(data = feature_scores, 
       aes(x = reorder(feature, -score),
           y = score)) +
  geom_bar(stat = "identity") +
  ggtitle(label = "Mushroom Features - Variable Importance in Random Forest") +
  labs(x = "Features", y = "Variable Importance Score") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+
  scale_y_continuous(breaks = pretty(1:feature_scores$score[1],10))

```


```{r, echo=FALSE}
# Reset ggplot theme -----------------------------------------------------------
theme_set(original_ggplot_theme)
```


### Conclusion

The data set was almost too perfect and the observations were too well separable into the different classes. This made machine learning methods and especially hyperparameter tuning almost unnecessary. Nevertheless, it was very interesting to look at this uncommon case of "perfect" data and it showed that you should critically question every analysis and not just blindly follow the standard machine learning procedure.
